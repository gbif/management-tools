# About crawling history

Datasets are crawled regularly to check for updates. These crawls can fail for various reasons and yypically this happens if the dataset is offline or responding incorrectly.  It can also happen when the GBIF systems are deliberately interrupted to e.g. deploy a new version.  The crawl history gives an overview of the crawls, which failed, how many occurrences failed during processing and how many were updated etc. This can be useful when diagnosing issues.  Additionally this can give insight into the growth or changes observed in the dataset over time.
